---
layout: default
title: Singer Identification
---
<div class='page-header title'>
  <div class='row'>
    <div class='span12'>
      <h1>Teaching Computers to Identify Singers</h1>
      <h1>
        <small>A Study in Signal Processing and Machine Learning</small>
      </h1>
    </div>
    <div class='span4'>
      <p>
        Northwestern University
        <br />
        EECS 349
        <br />
        Prof. Bryan Pardo
      </p>
    </div>
  </div>
</div>
<section id='team'>
  <h2>The Team</h2>
  <div class='row'>
    <div class='span4'>
      <div class='team-member'>
        <address>
          <img src='img/KaiSmall.jpg' />
          <a class='name' href='http://www.linkedin.com/pub/kai-hayashi/23/734/160'>Kai Hayashi</a>
          <a mailto=''>k.hermitian@gmail.com</a>
        </address>
      </div>
    </div>
    <div class='span4'>
      <div class='team-member'>
        <img src='img/cary_square.jpg' />
        <address>
          <a class='name' href='http://www.caryme.com'>Cary Lee</a>
          <br />
          <a mailto=''>carylee@gmail.com</a>
        </address>
      </div>
    </div>
    <div class='span4'>
      <div class='team-member'>
        <img src='img/DanHornSmall.jpg' />
        <address>
          <a class='name' href='http://linuxhaxor.blogspot.com/'>Daniel Myers</a>
          <br />
          <a mailto=''>dmritard96@gmail.com</a>
        </address>
      </div>
    </div>
    <div class='span4'>
      <div class='team-member'>
        <img src='img/BeckyPretty.jpg' />
        <address>
          <a class='name' href='http://www.linkedin.com/pub/rebecca-nevin/28/932/2b8'>Rebecca Nevin</a>
          <br />
          <a mailto=''>rlnevin@gmail.com</a>
        </address>
      </div>
    </div>
  </div>
</section>
<div class='row'>
  <div class='span16'>
    <section id='background'>
      <h2>Background</h2>
      <p>
        Identifying and differentiating the voices of individual singers is a
        task that comes naturally to humans. Consider the following recordings
        of two singers:
      </p>
      <div class='row'>
        <div class='span-one-third'>
          <h3>Singer 1</h3>
          <audio controls='controls'>
            <source src='audio/mezzo2-excerpt.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
        </div>
        <div class='span-one-third'>
          <h3>Singer 2</h3>
          <audio controls='controls'>
            <source src='audio/tenor1-excerpt.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
        </div>
      </div>
      <br />
      <p>
        Clearly, Singer 1 and Singer 2 are not the same person. Even after
        hearing only a short excerpt of each singer, we can distinguish between
        the two. We can deduce information about the singers, like their sexes or
        perhaps even their voice types. The task is somewhat more challenging with
        shorter samples given out of context, like the three below.
      </p>
      <div class='row'>
        <div class='span-one-third'>
          <h3>Mystery Singer 1</h3>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/Mezzo1/mid/a/a2.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
          <a data-placement='below' href='#background' rel='twipsy' title='Wildcard - not one of the singers above'>Who is it?</a>
        </div>
        <div class='span-one-third'>
          <h3>Mystery Singer 2</h3>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/tenor1/high/a/a3.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
          <a data-placement='below' href='#background' rel='twipsy' title='Singer 2'>Who is it?</a>
        </div>
        <div class='span-one-third'>
          <h3>Mystery Singer 3</h3>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/mezzo2/mid/a/a2.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
          <a data-placement='below' href='#background' rel='twipsy' title='Singer 1'>Who is it?</a>
        </div>
      </div>
      <br />
      <p>
        Even out of context, humans can usually differentiate between different
        singers' voices by the timbre, or quality, of the voice.  One of the
        most important components of what a human listener perceives as timbre
        is the spectral envelope of the singer's voice. The peaks of that to
        identify a singer is particularly useful because, as
        <a href='http://www.lma.cnrs-mrs.fr/~kronland/Sense_of_Sound/49690159.pdf'>Khine, Nwe, and Li</a>
        explain, "studies suggest that timbre is invariant with an individual
        singer."
      </p>
      <p>
        This project explores the application of machine learning to develop an automated singer identifier, largely following
        the methodology of
        <a href='http://home.earthlink.net/~bartscma/caruso.pdf'>Wakefield and Bartsch.</a>
        There many applications for this kind of automated singer identification, including:
      </p>
      <ul>
        <li>Source differentiation in audio recordings</li>
        <li>Identification of a singers presence on an unlabeled track.</li>
      </ul>
      <p>
        One of the problems in trying to develop such a classifier is the question of how to quantify
        the timbral features of an audio recording for training and classification. In this project, we compare
        the performacne of compositie transfer functions (CTFs) as proposed by
        <a href='http://home.earthlink.net/~bartscma/caruso.pdf'>Wakefield and Bartsch</a>
        and Mel frequency cepstral coefficients, as used by
        <a href='http://ismir2000.ismir.net/papers/logan_abs.pdf'>Logan.</a>
      </p>
    </section>
    <section id='dataset'>
      <h2>Data Set</h2>
      <p>
        We recruited volunteer singers to be recorded and included as part of our
        data set.  We recorded a total of 11 classically trained singers from
        Northwestern University Bienen School of Music consisting of 3 sopranos,
        4 mezzo-sopranos, 2 tenors and 2 baritones.
      </p>
      <p>
        Each singer was asked to sing:
      </p>
      <ul>
        <li>The first 5 notes of a major scale.</li>
        <li>Repeated in the low, middle and high regions of the singer’s range.</li>
        <li>Repeated for each of the 5 common Italian language vowels [a], [e], [i], [o], and [u]</li>
      </ul>
      <p>A single scale sounded like this (recorded at 44.1KHz, 16 bits/sample):</p>
      <audio controls='controls'>
        <source src='audio/tenor2-a.wav' type='audio/wav'></source>
        Your browser does not support the audio tag.
      </audio>
      <p>
        These scales were then manually split into single-ptched, 1-second samples, totalling 825 samples (75 per singer).
      </p>
      <a class='btn primary' href='https://github.com/downloads/crxpandion/Singer_ID/samples.zip'>Download our data set (66MB)</a>
    </section>
    <section id='signalprocessing'>
      <h2>Signal Processing</h2>
      <p>Before machine learning can occur, the necessary features must be extracted from the raw audio files.</p>
      <ol>
        <li>The Matlab Signal Processing toolbox was used to calculate a spectrogram for each signal.</li>
        <li>
          The formant was calculated using either an approximation of the CTF or code from
          <a href='http://labrosa.ee.columbia.edu/matlab/rastamat/'>Ellis</a>
          for generating MFCCs.
        </li>
        <li>Principle component analysis was performed on the outputs to generate data readable by</li>
        <a href='http://www.csie.ntu.edu.tw/~cjlin/libsvm/'>LibSVM.</a>
      </ol>
      <div class='media-grid'>
        <a href='img/bd.png'>
          <img src='img/bd.png' />
        </a>
      </div>
    </section>
    <section id='learningmethod'>
      <h2>Learning Method</h2>
      <ul>
        <li>
          The
          <a href='http://www.cs.waikato.ac.nz/ml/weka/index.html'>WEKA software package</a>
          was used to perform learning tasks.
        </li>
        <li>
          <a href='http://www.csie.ntu.edu.tw/~cjlin/libsvm/'>LibSVM</a>
          was used with quadratic classifiers.
        </li>
        <li>Ten-fold cross-validation.</li>
        <li>
          Samples were classified on three different levels:
          <ol>
            <li>Individual singers</li>
            <li>Voice Type</li>
            <li>Sex</li>
          </ol>
        </li>
        <li>Performance was measured as the percentage of correctly classified samples.</li>
      </ul>
    </section>
    <section id='results'>
      <h2>Results</h2>
      <h3>Composite Transfer Functions vs. Mel Frequency Cepstral Coefficients</h3>
      <ul class='media-grid confusion'>
        <li>
          <a data-placement='below' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Formant_all_Confusion.png' rel='twipsy' title='Confusion matrix for individual singers using CTF'>
            <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Formant_all_Confusion.png' />
          </a>
        </li>
        <li>
          <a data-placement='below' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_all_Confusion.png' rel='twipsy' title='Confusion matrix for individual singers using MFCC'>
            <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_all_Confusion.png' />
          </a>
        </li>
      </ul>
      <p>
        The plots above show confusion matrices for the CTF approximation
        method (left) and the MFCC method (right). It is clear that MFCCs
        were more successful at classifying our samples.
      </p>
      <h3>Classification Levels</h3>
      <div class='row boxplots'>
        <div class='span5'>
          <p>
            The box plots shown right summarize our learner’s ability to
            classify data samples at one of three levels:
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerClassBox.png'>individuals</a>
            (top left),
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerTypeClassBox.png'>voice type</a>
            (top right), and
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SexClassBox.png'>sex</a>
            (bottom left). As the classification
            becomes more general, the machine is more successful. There are
            several interesting conclusions to be drawn:
          </p>
          <ul>
            <li>Very good at identifying sex</li>
            <li>Voice type and individual identification are more challenging</li>
          </ul>
          <p>
            Further investigation revealed that the
            <strong>majority of voice type and individual errors occurred amongst the female samples,</strong>
            as can be seen in the
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_all_Confusion.png'>individual confusion matrix</a>
            and the
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_Type_Confusion.png'>voice type confusion matrix</a>
            (bottom right). The reasons for this are of
            great interest to us.
          </p>
        </div>
        <div class='span11'>
          <ul class='media-grid'>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerClassBox.png' rel='twipsy' title='Classifying Individual Singers'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerClassBox.png' />
              </a>
            </li>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerTypeClassBox.png' rel='twipsy' title='Classifying Voice Type'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerTypeClassBox.png' />
              </a>
            </li>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SexClassBox.png' rel='twipsy' title='Classifying Sex'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SexClassBox.png' />
              </a>
            </li>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_Type_Confusion.png' rel='twipsy' title='Classifying Voice Type'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_Type_Confusion.png' />
              </a>
            </li>
          </ul>
        </div>
      </div>
    </section>
    <section id='discussion'>
      <h2>Discussion</h2>
      <h3>Sparse spectra:</h3>
      <p>
        As
        <a href='http://home.earthlink.net/~bartscma/caruso.pdf'>Wakefield and Bartsch</a>
        explain, the female voice typically has a higher
        fundamental frequency, such that the spectral envelopes generated by the female
        singers are sparser than those of the male singers. These sparse spectra
        likely contribute to our difficulty in classifying female voices. The plot
        below shows the CTF of the two audio samples given for a soprano and a
        mezzo-soprano and demonstrates how the similarities could cause
        confusion.
      </p>
      <div class='row'>
        <div class='span7'>
          <h4>Soprano1</h4>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/Soprano1/mid/a/a2.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
          <h4>Mezzo1</h4>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/Mezzo1/mid/a/a1.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
        </div>
        <div class='span9 media-grid'>
          <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/CTF/comparisionMezzo1Soprano1CTF.png' rel='twipsy' title='Classifying Individual Singers'>
            <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/CTF/comparisionMezzo1Soprano1CTF.png' />
          </a>
        </div>
      </div>
      <h3>Voice maturity</h3>
      <p>
        The age and maturity of our sample set must also be considered in evaluating
        the results. A young woman’s voice can continue to change and develop into her
        30’s. As such, it is not uncommon for young female singers to change voice
        types early in their career. Our vocalists are all under the age of 30 and our
        youngest singer, Mezzo3, is currently considering transitioning to soprano
        repertoire. Thus our results are more vulnerable to voice type discrepancies
        than a sample set of fully developed vocalists.
      </p>
      <h3>Future research opportunities:</h3>
      <ol>
        <li>Could machines trained on matured singers help voice teachers identify the voice type of young students, particularly pre-mature females?</li>
        <li>How will a learner perform when the sample set contains more specific fachs such as coloratura soprano, lyric soprano, spinto soprano, and soubrette?</li>
      </ol>
    </section>
  </div>
</div>
