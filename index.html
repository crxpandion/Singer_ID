---
layout: default
title: Singer Identification
---
<div class='page-header'>
  <div class='row'>
    <div class='span12'>
      <h1>Teaching Computers to Identify Singers</h1>
      <h1>
        <small>A Study in Signal Processing and Machine Learning</small>
      </h1>
    </div>
    <div class='span4'>
      <p>
        Northwestern University
        <br />
        EECS 349
        <br />
        Bryan Pardo
      </p>
    </div>
  </div>
</div>
<section id='team'>
  <h2>The Team</h2>
  <div class='row'>
    <div class='span4'>
      <div class='team-member'>
        <address>
          <img src='img/KaiSmall.jpg' />
          <a class='name' href='http://www.linkedin.com/pub/kai-hayashi/23/734/160'>Kai Hayashi</a>
          <a mailto=''>k.hermitian@gmail.com</a>
        </address>
      </div>
    </div>
    <div class='span4'>
      <div class='team-member'>
        <img src='img/cary_square.jpg' />
        <address>
          <a class='name' href='http://www.caryme.com'>Cary Lee</a>
          <br />
          <a mailto=''>carylee@gmail.com</a>
        </address>
      </div>
    </div>
    <div class='span4'>
      <div class='team-member'>
        <img src='img/DanHornSmall.jpg' />
        <address>
          <a class='name' href='http://linuxhaxor.blogspot.com/'>Daniel Myers</a>
          <br />
          <a mailto=''>dmritard96@gmail.com</a>
        </address>
      </div>
    </div>
    <div class='span4'>
      <div class='team-member'>
        <img src='img/BeckyPretty.jpg' />
        <address>
          <a class='name' href='http://www.linkedin.com/pub/rebecca-nevin/28/932/2b8'>Rebecca Nevin</a>
          <br />
          <a mailto=''>rlnevin@gmail.com</a>
        </address>
      </div>
    </div>
  </div>
</section>
<div class='row'>
  <div class='span16'>
    <section id='background'>
      <h2>Background</h2>
      <ul>
        <li>Each voice is unique, therefore there are many applications for automated singer identification:</li>
        <li>Source differentiation in audio recordings</li>
        <li>Identification of a singers presence on an unlabeled track.</li>
        <li>Humans are able to naturally distinguish between different singers by listening for timbre. One component of timbre is the spectral envelope which has peaks called formants.</li>
        <li>
          <a href='http://home.earthlink.net/~bartscma/caruso.pdf'>Wakefield and Bartsch</a>
          have used composite transfer functions (CTFs) to extract formants and teach singer identification to machines.
        </li>
        <li>
          <a href='http://ismir2000.ismir.net/papers/logan_abs.pdf'>Logan</a>
          has used Mel frequency cepstral coefficients (MFCCs) as a tool for music modeling.
        </li>
      </ul>
    </section>
    <section id='dataset'>
      <h2>Data Set</h2>
      <ul>
        <li>A total of 11 classically trained vocalists from Northwestern University consisting of 3 sopranos, 4 mezzo-sopranos, 2 tenors and 2 baritones.</li>
        <li>825 total (75 per vocalist) single-pitch, 1-second samples with the following breakdown:</li>
        <li>The first 5 notes of a major scale.</li>
        <li>Repeated in the low, middle and high regions of the singer’s range.</li>
        <li>Repeated for each of the 5 common Italian language vowels.</li>
        <li>44.1KHz sampling rate and 16 bits/sample</li>
      </ul>
      <a class='btn primary' href='https://github.com/downloads/crxpandion/Singer_ID/samples.zip'>Download our data set (66MB)</a>
    </section>
    <section id='signalprocessing'>
      <h2>Signal Processing</h2>
      <p>Before machine learning can occur, the necessary features must be extracted from the raw audio files.</p>
      <ol>
        <li>The Matlab Signal Processing toolbox was used to calculate a spectrogram for each signal.</li>
        <li>The formant was calculated using either an approximation of the CTF or code from Ellis for generating MFCCs.</li>
        <li>Principle component analysis was performed on the outputs to generate data readable by</li>
        <a href='http://www.csie.ntu.edu.tw/~cjlin/libsvm/'>LibSVM.</a>
      </ol>
      <div class='media-grid'>
        <a href='img/bd.png'>
          <img src='img/bd.png' />
        </a>
      </div>
    </section>
    <section id='learningmethod'>
      <h2>Learning Method</h2>
      <ul>
        <li>
          The
          <a href='http://www.cs.waikato.ac.nz/ml/weka/index.html'>WEKA software package</a>
          was used to perform learning tasks.
        </li>
        <li>
          <a href='http://www.csie.ntu.edu.tw/~cjlin/libsvm/'>LibSVM</a>
          was used with quadratic classifiers.
        </li>
        <li>Ten-fold cross-validation.</li>
        <li>
          Samples were classified on three different levels:
          <ol>
            <li>Individual singers</li>
            <li>Voice Type</li>
            <li>Sex</li>
          </ol>
        </li>
        <li>Performance was measured as the percentage of correctly classified samples.</li>
      </ul>
    </section>
    <section id='results'>
      <h2>Results</h2>
      <h3>Composite Transfer Functions vs. Mel Frequency Cepstral Coefficients</h3>
      <ul class='media-grid confusion'>
        <li>
          <a data-placement='below' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Formant_all_Confusion.png' rel='twipsy' title='Confusion matrix for individual singers using CTF'>
            <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Formant_all_Confusion.png' />
          </a>
        </li>
        <li>
          <a data-placement='below' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_all_Confusion.png' rel='twipsy' title='Confusion matrix for individual singers using MFCC'>
            <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_all_Confusion.png' />
          </a>
        </li>
      </ul>
      <p>
        The plots above show confusion matrices for the CTF approximation
        method (left) and the MFCC method (right). It is clear that MFCCs
        were more successful at classifying our samples.
      </p>
      <h3>Classification Levels</h3>
      <div class='row boxplots'>
        <div class='span5'>
          <p>
            The box plots shown right summarize our learner’s ability to
            classify data samples at one of three levels:
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerClassBox.png'>individuals</a>
            (top left),
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerTypeClassBox.png'>voice type</a>
            (top right), and
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SexClassBox.png'>sex</a>
            (bottom left). As the classification
            becomes more general, the machine is more successful. There are
            several interesting conclusions to be drawn:
          </p>
          <ul>
            <li>Very good at identifying sex</li>
            <li>Voice type and individual identification are more challenging</li>
          </ul>
          <p>
            Further investigation revealed that the
            <strong>majority of voice type and individual errors occurred amongst the female samples,</strong>
            as can be seen in the
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_all_Confusion.png'>individual confusion matrix</a>
            and the
            <a href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_Type_Confusion.png'>voice type confusion matrix</a>
            (bottom right). The reasons for this are of
            great interest to us.
          </p>
        </div>
        <div class='span11'>
          <ul class='media-grid'>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerClassBox.png' rel='twipsy' title='Classifying Individual Singers'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerClassBox.png' />
              </a>
            </li>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerTypeClassBox.png' rel='twipsy' title='Classifying Voice Type'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SingerTypeClassBox.png' />
              </a>
            </li>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SexClassBox.png' rel='twipsy' title='Classifying Sex'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/boxplots/SexClassBox.png' />
              </a>
            </li>
            <li>
              <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_Type_Confusion.png' rel='twipsy' title='Classifying Voice Type'>
                <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/Confusion/Cepstral_Type_Confusion.png' />
              </a>
            </li>
          </ul>
        </div>
      </div>
    </section>
    <section id='discussion'>
      <h2>Discussion</h2>
      <h3>Sparse spectra:</h3>
      <p>
        As
        <a href='http://home.earthlink.net/~bartscma/caruso.pdf'>Wakefield and Bartsch</a>
        explain, the female voice typically has a higher
        fundamental frequency, such that the spectral envelopes generated by the female
        singers are sparser than those of the male singers. These sparse spectra
        likely contribute to our difficulty in classifying female voices. The plot
        below shows the CTF of the two audio samples given for a soprano and a
        mezzo-soprano and demonstrates how the similarities could cause
        confusion.
      </p>
      <div class='row'>
        <div class='span7'>
          <h4>Soprano1</h4>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/Soprano1/mid/a/a2.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
          <h4>Mezzo1</h4>
          <audio controls='controls'>
            <source src='https://github.com/crxpandion/Singer_ID/raw/master/samples/Mezzo1/mid/a/a1.wav' type='audio/wav'></source>
            Your browser does not support the audio tag.
          </audio>
        </div>
        <div class='span9 media-grid'>
          <a data-placement='left' href='https://github.com/crxpandion/Singer_ID/raw/master/figures/CTF/comparisionMezzo1Soprano1CTF.png' rel='twipsy' title='Classifying Individual Singers'>
            <img src='https://github.com/crxpandion/Singer_ID/raw/master/figures/CTF/comparisionMezzo1Soprano1CTF.png' />
          </a>
        </div>
      </div>
      <h3>Voice maturity</h3>
      <p>
        The age and maturity of our sample set must also be considered in evaluating
        the results. A young woman’s voice can continue to change and develop into her
        30’s. As such, it is not uncommon for young female singers to change voice
        types early in their career. Our vocalists are all under the age of 30 and our
        youngest singer, Mezzo3, is currently considering transitioning to soprano
        repertoire. Thus our results are more vulnerable to voice type discrepancies
        than a sample set of fully developed vocalists.
      </p>
      <h3>Future research opportunities:</h3>
      <ol>
        <li>Could machines trained on matured singers help voice teachers identify the voice type of young students, particularly pre-mature females?</li>
        <li>How will a learner perform when the sample set contains more specific fachs such as coloratura soprano, lyric soprano, spinto soprano, and soubrette?</li>
      </ol>
    </section>
  </div>
</div>
